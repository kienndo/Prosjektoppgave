{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44714caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kiennd\\AppData\\Local\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from inference import StableEmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ea556c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CKPT_PATH = \"model.ckpt\"\n",
    "NUM_NEW_CLASSES = 10\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac95c193",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f51ef2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_state = torch.load(CKPT_PATH, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f903c723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ViT backbone: beitv2_base_patch16_224.in1k_ft_in22k_in1k\n",
      "StableEmbeddingModel initialized with ViT backbone: beitv2_base_patch16_224.in1k_ft_in22k_in1k\n",
      "  Embedding Dim: 512, Num Classes: 10\n",
      "  ArcFace s: 64.0, m: 0.5\n",
      "  Backbone out features (ViT embed_dim): 768\n",
      "  BN in embedding: False, Dropout in embedding: 0.11\n",
      "Missing keys (expected: arcface_head.*): ['arcface_head.weight', 'arcface_head.cos_m', 'arcface_head.sin_m', 'arcface_head.th', 'arcface_head.mm', 'arcface_head.eps']\n",
      "Unexpected keys: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StableEmbeddingModel(\n",
       "  (backbone): Beit(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): Identity()\n",
       "    (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (head_drop): Dropout(p=0.0, inplace=False)\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (pooling): ViTAttentionPooling(\n",
       "    (attention_net): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=192, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (embedding_fc): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "  )\n",
       "  (arcface_head): ArcFaceHead()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = StableEmbeddingModel(\n",
    "    embedding_dim=512,\n",
    "    num_classes=NUM_NEW_CLASSES,\n",
    "    pretrained_backbone=True,\n",
    ")\n",
    "\n",
    "filtered_state = {\n",
    "    k: v for k, v in old_state.items()\n",
    "    if not k.startswith(\"arcface_head.\")\n",
    "}\n",
    "\n",
    "missing, unexpected = model.load_state_dict(filtered_state, strict=False)\n",
    "print(\"Missing keys (expected: arcface_head.*):\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"backbone\"):\n",
    "        param.requires_grad = False\n",
    "        \n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d859530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=LR,\n",
    "    weight_decay=1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babc3220",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FishDataset(Dataset):\n",
    "    def __init__(self, image_root, annotations, transform=None):\n",
    "        \"\"\"\n",
    "        image_root: folder with images\n",
    "        annotations: list of (image_filename, class_id)\n",
    "        \"\"\"\n",
    "        self.image_root = image_root\n",
    "        self.samples = annotations\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, label = self.samples[idx]\n",
    "        img_path = os.path.join(self.image_root, img_name)\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7061484",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annot = pd.read_csv(\"my_annotations.csv\")\n",
    "\n",
    "label_to_id = {label: idx for idx, label in enumerate(sorted(df_annot[\"label\"].unique()))}\n",
    "\n",
    "train_annotations = [\n",
    "    (row[\"image_file\"], label_to_id[row[\"label\"]]) \n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "val_annotations = []\n",
    "\n",
    "train_dataset = FishDataset(\"data_images/train\", train_annotations)\n",
    "val_dataset   = FishDataset(\"data_images/val\",   val_annotations)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be8723a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        # Manual forward using modelâ€™s components\n",
    "        # 1) ViT features\n",
    "        features = model.backbone_feature_extractor(images)  # [B, N+1, D]\n",
    "\n",
    "        # 2) Remove CLS token, keep patches\n",
    "        if hasattr(model.backbone, \"cls_token\"):\n",
    "            patch_tokens = features[:, 1:, :]\n",
    "        else:\n",
    "            patch_tokens = features\n",
    "\n",
    "        # 3) Attention pooling\n",
    "        pooled, _ = model.pooling(patch_tokens, return_attention_map=True)\n",
    "\n",
    "        # 4) Embedding head + L2-normalization\n",
    "        emb_raw = model.embedding_fc(pooled)\n",
    "        emb_norm = F.normalize(emb_raw, p=2, dim=1)\n",
    "\n",
    "        # 5) ArcFace logits (margin injected using labels)\n",
    "        logits = model.arcface_head(emb_norm, labels)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_samples += images.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples if total_samples > 0 else 0.0\n",
    "    acc = total_correct / total_samples if total_samples > 0 else 0.0\n",
    "    print(f\"Epoch {epoch} - Train loss: {avg_loss:.4f}, acc: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d951982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        features = model.backbone_feature_extractor(images)\n",
    "        if hasattr(model.backbone, \"cls_token\"):\n",
    "            patch_tokens = features[:, 1:, :]\n",
    "        else:\n",
    "            patch_tokens = features\n",
    "\n",
    "        pooled, _ = model.pooling(patch_tokens, return_attention_map=True)\n",
    "        emb_raw = model.embedding_fc(pooled)\n",
    "        emb_norm = F.normalize(emb_raw, p=2, dim=1)\n",
    "        logits = model.arcface_head(emb_norm, labels)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_samples += images.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples if total_samples > 0 else 0.0\n",
    "    acc = total_correct / total_samples if total_samples > 0 else 0.0\n",
    "    print(f\"Epoch {epoch} - Val loss: {avg_loss:.4f}, acc: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947f612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_one_epoch(model, train_loader, optimizer, epoch)\n",
    "    validate(model, val_loader, epoch)\n",
    "\n",
    "# Save your new transfer-learned weights\n",
    "torch.save(model.state_dict(), \"model_transfer_newdataset.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
